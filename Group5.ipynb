{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Group5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP0PWJuSnNtBJAlnDSbjv7s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lopeselio/CSE4022-Group5-Activity/blob/master/Group5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rq8Fqqsqchfh",
        "outputId": "69170bfc-6719-4ca8-ceea-5cc3b5bcd7e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"all\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyg2936mehAE"
      },
      "source": [
        "import gensim"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8-4PnMMfstn",
        "outputId": "c4175185-918c-44fa-9552-de699dc78ad5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1pJaL6FgAYg",
        "outputId": "3d672078-7f14-467f-f863-6d42c6304a3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "with open('/content/drive/My Drive/1.txt', 'r') as f: \n",
        "    print(f.read())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Anniversary editions have the feel\n",
            "of a graduation:a year of studious\n",
            "slogging(of which,truth be told.\n",
            "my team and l do very little)and\n",
            "madcap fun(which we only wish we could\n",
            "indulge in more)rounded off with a sense of\n",
            "achievement and lingering anxiety.There's\n",
            "pride that National Geographic Traveller India\n",
            "has lived to see another day,and in today's\n",
            "precarious media landscape,that should\n",
            "account for something.Then the gnawing\n",
            "question:did we get it right?\n",
            "When it comes to travel,is there a right or a\n",
            "wrong way to do it?Early this month,The New\n",
            "York Times unearthed Albert Einstein's entries\n",
            "of his journeys around Asia and dliscovered\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_GDQ4NkgLV8",
        "outputId": "1d5435ff-4c38-4394-d2f3-2bac7e553ab0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "with open('/content/drive/My Drive/1.txt', 'r') as f: \n",
        "    print(f.read())\n",
        "    print(\"==========\")\n",
        "with open('/content/drive/My Drive/2.txt', 'r') as g: \n",
        "    print(g.read())\n",
        "    print(\"==========\")\n",
        "with open('/content/drive/My Drive/3.txt', 'r') as h: \n",
        "    print(h.read())\n",
        "    print(\"==========\")\n",
        "with open('/content/drive/My Drive/4.txt', 'r') as i: \n",
        "    print(i.read())\n",
        "    print(\"==========\")\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Anniversary editions have the feel\n",
            "of a graduation:a year of studious\n",
            "slogging(of which,truth be told.\n",
            "my team and l do very little)and\n",
            "madcap fun(which we only wish we could\n",
            "indulge in more)rounded off with a sense of\n",
            "achievement and lingering anxiety.There's\n",
            "pride that National Geographic Traveller India\n",
            "has lived to see another day,and in today's\n",
            "precarious media landscape,that should\n",
            "account for something.Then the gnawing\n",
            "question:did we get it right?\n",
            "When it comes to travel,is there a right or a\n",
            "wrong way to do it?Early this month,The New\n",
            "York Times unearthed Albert Einstein's entries\n",
            "of his journeys around Asia and dliscovered\n",
            "==========\n",
            "Our year-end edition toasts ultra-\n",
            "indulgence while travelling.\n",
            "featuring itineraries that many will\n",
            "know to be out of their financial\n",
            "reach.In producing these narratives,I was\n",
            "struck by a contrast.\n",
            "Travel today is dominated by minimalists\n",
            "or downsizers,those who preach the gospel\n",
            "of\"hard-knock wanderlust.\"And they almost\n",
            "always reap universal admiration.They are\n",
            "characters to aspire to.examples of made-\n",
            "for-Instagram sayings such as,\"All you need\n",
            "is a backpack\"or\"#MotorcycleDiaries.\"\n",
            "Unable to join these gallivanting philosophers.\n",
            "others marvel at their brave rebellion-oh.to\n",
            "give up the predictability of overpriced tourist\n",
            "traps someday.they sigh.\n",
            "In this context,luxury travel evokes a\n",
            "Molotov cocktail of teelings A billionaire on\n",
            "==========\n",
            "For my money,memorable disagree-\n",
            "ments often centre on food.A friend\n",
            "who was about to settle abroad was\n",
            "feeling particularly wistful\n",
            "about a storied south Bombay restaurant,the kind\n",
            "of eatery that locals like to call“overrated\"\n",
            "and guidebook-toting tourists faithfully make\n",
            "a beeline for.His favourite on the menu?The\n",
            "baklava-a dry fruit-laden traditional sweet\n",
            "that smacked of decadence in every bite.\n",
            "The first time he requested for the dessert\n",
            "==========\n",
            "I like a bit of pow-wow in any place.Let me \n",
            "rephrase before you think I am eternally\n",
            "hankering for a fight.What I mean is\n",
            "I would choose crooked streets over\n",
            "straight highways,sweaty mayhem over\n",
            "pristine elegance.This is why no matter\n",
            "where I go in this world,coming home to\n",
            "India,and especially Bombay,is never dull.l\n",
            "blame growing up in the city for my pugilistic\n",
            "predilections.One of the many descriptors\n",
            "that Mark Twain used in relation to Bombay\n",
            "was\"pow-wow.\"The place seemed to\n",
            "confound him:\"Bewitching\",\"Bewildering\n",
            "＂Enchanting\",\"Arabian Nights come again?\"-\n",
            "the man was repulsed and riveted at the same\n",
            "time.It was a place befitting the number of\n",
            "exclamations he used.\n",
            "\n",
            "==========\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekhGV_egksw1",
        "outputId": "b4b9b32e-275e-4a52-842f-7d21925e656c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "a = open('1.txt',\"r\").readlines()\n",
        "print(a)\n",
        "print(\"===\")\n",
        "a1 = \" \".join([x.strip() for x in a])\n",
        "print(a1)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Anniversary editions have the feel\\n', 'of a graduation:a year of studious\\n', 'slogging(of which,truth be told.\\n', 'my team and l do very little)and\\n', 'madcap fun(which we only wish we could\\n', 'indulge in more)rounded off with a sense of\\n', \"achievement and lingering anxiety.There's\\n\", 'pride that National Geographic Traveller India\\n', \"has lived to see another day,and in today's\\n\", 'precarious media landscape,that should\\n', 'account for something.Then the gnawing\\n', 'question:did we get it right?\\n', 'When it comes to travel,is there a right or a\\n', 'wrong way to do it?Early this month,The New\\n', \"York Times unearthed Albert Einstein's entries\\n\", 'of his journeys around Asia and dliscovered']\n",
            "===\n",
            "Anniversary editions have the feel of a graduation:a year of studious slogging(of which,truth be told. my team and l do very little)and madcap fun(which we only wish we could indulge in more)rounded off with a sense of achievement and lingering anxiety.There's pride that National Geographic Traveller India has lived to see another day,and in today's precarious media landscape,that should account for something.Then the gnawing question:did we get it right? When it comes to travel,is there a right or a wrong way to do it?Early this month,The New York Times unearthed Albert Einstein's entries of his journeys around Asia and dliscovered\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLAjt8_DlUjQ",
        "outputId": "fce40709-7c1d-4c2c-cb11-0b087634ca6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "b = open('2.txt',\"r\").readlines()\n",
        "print(b)\n",
        "b1 = \" \".join([x.strip() for x in b])\n",
        "print(b1)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Our year-end edition toasts ultra-\\n', 'indulgence while travelling.\\n', 'featuring itineraries that many will\\n', 'know to be out of their financial\\n', 'reach.In producing these narratives,I was\\n', 'struck by a contrast.\\n', 'Travel today is dominated by minimalists\\n', 'or downsizers,those who preach the gospel\\n', 'of\"hard-knock wanderlust.\"And they almost\\n', 'always reap universal admiration.They are\\n', 'characters to aspire to.examples of made-\\n', 'for-Instagram sayings such as,\"All you need\\n', 'is a backpack\"or\"#MotorcycleDiaries.\"\\n', 'Unable to join these gallivanting philosophers.\\n', 'others marvel at their brave rebellion-oh.to\\n', 'give up the predictability of overpriced tourist\\n', 'traps someday.they sigh.\\n', 'In this context,luxury travel evokes a\\n', 'Molotov cocktail of teelings A billionaire on']\n",
            "Our year-end edition toasts ultra- indulgence while travelling. featuring itineraries that many will know to be out of their financial reach.In producing these narratives,I was struck by a contrast. Travel today is dominated by minimalists or downsizers,those who preach the gospel of\"hard-knock wanderlust.\"And they almost always reap universal admiration.They are characters to aspire to.examples of made- for-Instagram sayings such as,\"All you need is a backpack\"or\"#MotorcycleDiaries.\" Unable to join these gallivanting philosophers. others marvel at their brave rebellion-oh.to give up the predictability of overpriced tourist traps someday.they sigh. In this context,luxury travel evokes a Molotov cocktail of teelings A billionaire on\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WfDp65OldzB",
        "outputId": "d0e5573c-4a70-48c9-8b87-d671e9bdb011",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "c = open('3.txt',\"r\").readlines()\n",
        "print(c)\n",
        "c1 = \" \".join([x.strip() for x in c])\n",
        "print(c1)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['For my money,memorable disagree-\\n', 'ments often centre on food.A friend\\n', 'who was about to settle abroad was\\n', 'feeling particularly wistful\\n', 'about a storied south Bombay restaurant,the kind\\n', 'of eatery that locals like to call“overrated\"\\n', 'and guidebook-toting tourists faithfully make\\n', 'a beeline for.His favourite on the menu?The\\n', 'baklava-a dry fruit-laden traditional sweet\\n', 'that smacked of decadence in every bite.\\n', 'The first time he requested for the dessert']\n",
            "For my money,memorable disagree- ments often centre on food.A friend who was about to settle abroad was feeling particularly wistful about a storied south Bombay restaurant,the kind of eatery that locals like to call“overrated\" and guidebook-toting tourists faithfully make a beeline for.His favourite on the menu?The baklava-a dry fruit-laden traditional sweet that smacked of decadence in every bite. The first time he requested for the dessert\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgXIw67tl6l1",
        "outputId": "6331e79b-0ba1-47ac-ece8-94677c526d69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "d = open('4.txt',\"r\").readlines()\n",
        "print(d)\n",
        "d1 = \" \".join([x.strip() for x in d])\n",
        "print(d1)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I like a bit of pow-wow in any place.Let me \\n', 'rephrase before you think I am eternally\\n', 'hankering for a fight.What I mean is\\n', 'I would choose crooked streets over\\n', 'straight highways,sweaty mayhem over\\n', 'pristine elegance.This is why no matter\\n', 'where I go in this world,coming home to\\n', 'India,and especially Bombay,is never dull.l\\n', 'blame growing up in the city for my pugilistic\\n', 'predilections.One of the many descriptors\\n', 'that Mark Twain used in relation to Bombay\\n', 'was\"pow-wow.\"The place seemed to\\n', 'confound him:\"Bewitching\",\"Bewildering\\n', '＂Enchanting\",\"Arabian Nights come again?\"-\\n', 'the man was repulsed and riveted at the same\\n', 'time.It was a place befitting the number of\\n', 'exclamations he used.\\n']\n",
            "I like a bit of pow-wow in any place.Let me rephrase before you think I am eternally hankering for a fight.What I mean is I would choose crooked streets over straight highways,sweaty mayhem over pristine elegance.This is why no matter where I go in this world,coming home to India,and especially Bombay,is never dull.l blame growing up in the city for my pugilistic predilections.One of the many descriptors that Mark Twain used in relation to Bombay was\"pow-wow.\"The place seemed to confound him:\"Bewitching\",\"Bewildering ＂Enchanting\",\"Arabian Nights come again?\"- the man was repulsed and riveted at the same time.It was a place befitting the number of exclamations he used.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey0HjGinqYSW"
      },
      "source": [
        "# *Generate Tokens*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOoLmcJYqfoq"
      },
      "source": [
        "### 1.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ge-wKYbn7he",
        "outputId": "a3a1504b-6d2f-4db0-8c3d-4aa44a3e44dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "file_docs1 = []\n",
        "\n",
        "with open ('1.txt') as f:\n",
        "    tokens = sent_tokenize(f.read())\n",
        "    for line in tokens:\n",
        "        file_docs1.append(line)\n",
        "\n",
        "print(\"Number of documents:\",len(file_docs1))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of documents: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctf911hYpRVI",
        "outputId": "815290a1-cc60-4081-c207-a638b4245c13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print(file_docs1)\n",
        "gen_docs1 = [[w.lower() for w in word_tokenize(text)] \n",
        "            for text in file_docs1]\n",
        "print(gen_docs1)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Anniversary editions have the feel\\nof a graduation:a year of studious\\nslogging(of which,truth be told.', \"my team and l do very little)and\\nmadcap fun(which we only wish we could\\nindulge in more)rounded off with a sense of\\nachievement and lingering anxiety.There's\\npride that National Geographic Traveller India\\nhas lived to see another day,and in today's\\nprecarious media landscape,that should\\naccount for something.Then the gnawing\\nquestion:did we get it right?\", \"When it comes to travel,is there a right or a\\nwrong way to do it?Early this month,The New\\nYork Times unearthed Albert Einstein's entries\\nof his journeys around Asia and dliscovered\"]\n",
            "[['anniversary', 'editions', 'have', 'the', 'feel', 'of', 'a', 'graduation', ':', 'a', 'year', 'of', 'studious', 'slogging', '(', 'of', 'which', ',', 'truth', 'be', 'told', '.'], ['my', 'team', 'and', 'l', 'do', 'very', 'little', ')', 'and', 'madcap', 'fun', '(', 'which', 'we', 'only', 'wish', 'we', 'could', 'indulge', 'in', 'more', ')', 'rounded', 'off', 'with', 'a', 'sense', 'of', 'achievement', 'and', 'lingering', \"anxiety.there's\", 'pride', 'that', 'national', 'geographic', 'traveller', 'india', 'has', 'lived', 'to', 'see', 'another', 'day', ',', 'and', 'in', \"today's\", 'precarious', 'media', 'landscape', ',', 'that', 'should', 'account', 'for', 'something.then', 'the', 'gnawing', 'question', ':', 'did', 'we', 'get', 'it', 'right', '?'], ['when', 'it', 'comes', 'to', 'travel', ',', 'is', 'there', 'a', 'right', 'or', 'a', 'wrong', 'way', 'to', 'do', 'it', '?', 'early', 'this', 'month', ',', 'the', 'new', 'york', 'times', 'unearthed', 'albert', 'einstein', \"'s\", 'entries', 'of', 'his', 'journeys', 'around', 'asia', 'and', 'dliscovered']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap67wIGLqlKl"
      },
      "source": [
        "## 2.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0XrxN-8qomt",
        "outputId": "2747d6bb-74fb-4eaf-e936-fc417832bddc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "file_docs2 = []\n",
        "\n",
        "with open ('2.txt') as f:\n",
        "    tokens = sent_tokenize(f.read())\n",
        "    for line in tokens:\n",
        "        file_docs2.append(line)\n",
        "\n",
        "print(\"Number of documents:\",len(file_docs2))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of documents: 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ6lPr1Hq2_G",
        "outputId": "163f6687-2a41-4b41-a115-b22aa96ff350",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print(file_docs2)\n",
        "gen_docs2 = [[w.lower() for w in word_tokenize(text)] \n",
        "            for text in file_docs2]\n",
        "print(gen_docs2)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Our year-end edition toasts ultra-\\nindulgence while travelling.', 'featuring itineraries that many will\\nknow to be out of their financial\\nreach.In producing these narratives,I was\\nstruck by a contrast.', 'Travel today is dominated by minimalists\\nor downsizers,those who preach the gospel\\nof\"hard-knock wanderlust.', '\"And they almost\\nalways reap universal admiration.They are\\ncharacters to aspire to.examples of made-\\nfor-Instagram sayings such as,\"All you need\\nis a backpack\"or\"#MotorcycleDiaries.\"', 'Unable to join these gallivanting philosophers.', 'others marvel at their brave rebellion-oh.to\\ngive up the predictability of overpriced tourist\\ntraps someday.they sigh.', 'In this context,luxury travel evokes a\\nMolotov cocktail of teelings A billionaire on']\n",
            "[['our', 'year-end', 'edition', 'toasts', 'ultra-', 'indulgence', 'while', 'travelling', '.'], ['featuring', 'itineraries', 'that', 'many', 'will', 'know', 'to', 'be', 'out', 'of', 'their', 'financial', 'reach.in', 'producing', 'these', 'narratives', ',', 'i', 'was', 'struck', 'by', 'a', 'contrast', '.'], ['travel', 'today', 'is', 'dominated', 'by', 'minimalists', 'or', 'downsizers', ',', 'those', 'who', 'preach', 'the', 'gospel', 'of', \"''\", 'hard-knock', 'wanderlust', '.'], ['``', 'and', 'they', 'almost', 'always', 'reap', 'universal', 'admiration.they', 'are', 'characters', 'to', 'aspire', 'to.examples', 'of', 'made-', 'for-instagram', 'sayings', 'such', 'as', ',', \"''\", 'all', 'you', 'need', 'is', 'a', 'backpack', \"''\", 'or', \"''\", '#', 'motorcyclediaries', '.', \"''\"], ['unable', 'to', 'join', 'these', 'gallivanting', 'philosophers', '.'], ['others', 'marvel', 'at', 'their', 'brave', 'rebellion-oh.to', 'give', 'up', 'the', 'predictability', 'of', 'overpriced', 'tourist', 'traps', 'someday.they', 'sigh', '.'], ['in', 'this', 'context', ',', 'luxury', 'travel', 'evokes', 'a', 'molotov', 'cocktail', 'of', 'teelings', 'a', 'billionaire', 'on']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je_77HKGq_5K"
      },
      "source": [
        "## 3.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TPVeRAvrB1e",
        "outputId": "e6cc9d33-0f1e-4b54-e16d-60dd9eccaa0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "file_docs3 = []\n",
        "\n",
        "with open ('3.txt') as f:\n",
        "    tokens = sent_tokenize(f.read())\n",
        "    for line in tokens:\n",
        "        file_docs3.append(line)\n",
        "\n",
        "print(\"Number of documents:\",len(file_docs3))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of documents: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWxFyjp8rI-8",
        "outputId": "6d98357f-9e61-4f59-f931-caa67f776f09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print(file_docs3)\n",
        "gen_docs3 = [[w.lower() for w in word_tokenize(text)] \n",
        "            for text in file_docs3]\n",
        "print(gen_docs3)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['For my money,memorable disagree-\\nments often centre on food.A friend\\nwho was about to settle abroad was\\nfeeling particularly wistful\\nabout a storied south Bombay restaurant,the kind\\nof eatery that locals like to call“overrated\"\\nand guidebook-toting tourists faithfully make\\na beeline for.His favourite on the menu?The\\nbaklava-a dry fruit-laden traditional sweet\\nthat smacked of decadence in every bite.', 'The first time he requested for the dessert']\n",
            "[['for', 'my', 'money', ',', 'memorable', 'disagree-', 'ments', 'often', 'centre', 'on', 'food.a', 'friend', 'who', 'was', 'about', 'to', 'settle', 'abroad', 'was', 'feeling', 'particularly', 'wistful', 'about', 'a', 'storied', 'south', 'bombay', 'restaurant', ',', 'the', 'kind', 'of', 'eatery', 'that', 'locals', 'like', 'to', 'call', '“', 'overrated', \"''\", 'and', 'guidebook-toting', 'tourists', 'faithfully', 'make', 'a', 'beeline', 'for.his', 'favourite', 'on', 'the', 'menu', '?', 'the', 'baklava-a', 'dry', 'fruit-laden', 'traditional', 'sweet', 'that', 'smacked', 'of', 'decadence', 'in', 'every', 'bite', '.'], ['the', 'first', 'time', 'he', 'requested', 'for', 'the', 'dessert']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e0yZcVprRGt"
      },
      "source": [
        "### 4.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2ca6GVqrTSs",
        "outputId": "9dd3681b-a341-44c2-8e79-a2df6e2fa4ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "file_docs4 = []\n",
        "\n",
        "with open ('4.txt') as f:\n",
        "    tokens = sent_tokenize(f.read())\n",
        "    for line in tokens:\n",
        "        file_docs4.append(line)\n",
        "\n",
        "print(\"Number of documents:\",len(file_docs4))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of documents: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YujGXERlrb1B",
        "outputId": "ca00a9a7-1346-41c1-e171-8030c0e33c86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print(file_docs4)\n",
        "gen_docs4 = [[w.lower() for w in word_tokenize(text)] \n",
        "            for text in file_docs4]\n",
        "print(gen_docs4)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I like a bit of pow-wow in any place.Let me \\nrephrase before you think I am eternally\\nhankering for a fight.What I mean is\\nI would choose crooked streets over\\nstraight highways,sweaty mayhem over\\npristine elegance.This is why no matter\\nwhere I go in this world,coming home to\\nIndia,and especially Bombay,is never dull.l\\nblame growing up in the city for my pugilistic\\npredilections.One of the many descriptors\\nthat Mark Twain used in relation to Bombay\\nwas\"pow-wow.', '\"The place seemed to\\nconfound him:\"Bewitching\",\"Bewildering\\n＂Enchanting\",\"Arabian Nights come again?', '\"-\\nthe man was repulsed and riveted at the same\\ntime.It was a place befitting the number of\\nexclamations he used.']\n",
            "[['i', 'like', 'a', 'bit', 'of', 'pow-wow', 'in', 'any', 'place.let', 'me', 'rephrase', 'before', 'you', 'think', 'i', 'am', 'eternally', 'hankering', 'for', 'a', 'fight.what', 'i', 'mean', 'is', 'i', 'would', 'choose', 'crooked', 'streets', 'over', 'straight', 'highways', ',', 'sweaty', 'mayhem', 'over', 'pristine', 'elegance.this', 'is', 'why', 'no', 'matter', 'where', 'i', 'go', 'in', 'this', 'world', ',', 'coming', 'home', 'to', 'india', ',', 'and', 'especially', 'bombay', ',', 'is', 'never', 'dull.l', 'blame', 'growing', 'up', 'in', 'the', 'city', 'for', 'my', 'pugilistic', 'predilections.one', 'of', 'the', 'many', 'descriptors', 'that', 'mark', 'twain', 'used', 'in', 'relation', 'to', 'bombay', 'was', \"''\", 'pow-wow', '.'], ['``', 'the', 'place', 'seemed', 'to', 'confound', 'him', ':', \"''\", 'bewitching', \"''\", ',', \"''\", 'bewildering', '＂enchanting', \"''\", ',', \"''\", 'arabian', 'nights', 'come', 'again', '?'], ['``', '-', 'the', 'man', 'was', 'repulsed', 'and', 'riveted', 'at', 'the', 'same', 'time.it', 'was', 'a', 'place', 'befitting', 'the', 'number', 'of', 'exclamations', 'he', 'used', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew6kjC69rmvQ"
      },
      "source": [
        "# ============================================================================================================================================== "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYIcMnkIsAe2"
      },
      "source": [
        "## Making a dictionary of tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "batJmHLmrvyd",
        "outputId": "337abe05-3dc0-4d8b-dad1-99fd2e90b71e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# 1.txt\n",
        "dictionary1 = gensim.corpora.Dictionary(gen_docs1)\n",
        "print(dictionary1.token2id)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'(': 0, ',': 1, '.': 2, ':': 3, 'a': 4, 'anniversary': 5, 'be': 6, 'editions': 7, 'feel': 8, 'graduation': 9, 'have': 10, 'of': 11, 'slogging': 12, 'studious': 13, 'the': 14, 'told': 15, 'truth': 16, 'which': 17, 'year': 18, ')': 19, '?': 20, 'account': 21, 'achievement': 22, 'and': 23, 'another': 24, \"anxiety.there's\": 25, 'could': 26, 'day': 27, 'did': 28, 'do': 29, 'for': 30, 'fun': 31, 'geographic': 32, 'get': 33, 'gnawing': 34, 'has': 35, 'in': 36, 'india': 37, 'indulge': 38, 'it': 39, 'l': 40, 'landscape': 41, 'lingering': 42, 'little': 43, 'lived': 44, 'madcap': 45, 'media': 46, 'more': 47, 'my': 48, 'national': 49, 'off': 50, 'only': 51, 'precarious': 52, 'pride': 53, 'question': 54, 'right': 55, 'rounded': 56, 'see': 57, 'sense': 58, 'should': 59, 'something.then': 60, 'team': 61, 'that': 62, 'to': 63, \"today's\": 64, 'traveller': 65, 'very': 66, 'we': 67, 'wish': 68, 'with': 69, \"'s\": 70, 'albert': 71, 'around': 72, 'asia': 73, 'comes': 74, 'dliscovered': 75, 'early': 76, 'einstein': 77, 'entries': 78, 'his': 79, 'is': 80, 'journeys': 81, 'month': 82, 'new': 83, 'or': 84, 'there': 85, 'this': 86, 'times': 87, 'travel': 88, 'unearthed': 89, 'way': 90, 'when': 91, 'wrong': 92, 'york': 93}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn6qbue7scQY",
        "outputId": "2291f08a-4109-4578-d04e-aff7c4f34dc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# 2.txt\n",
        "dictionary2 = gensim.corpora.Dictionary(gen_docs2)\n",
        "print(dictionary2.token2id)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'.': 0, 'edition': 1, 'indulgence': 2, 'our': 3, 'toasts': 4, 'travelling': 5, 'ultra-': 6, 'while': 7, 'year-end': 8, ',': 9, 'a': 10, 'be': 11, 'by': 12, 'contrast': 13, 'featuring': 14, 'financial': 15, 'i': 16, 'itineraries': 17, 'know': 18, 'many': 19, 'narratives': 20, 'of': 21, 'out': 22, 'producing': 23, 'reach.in': 24, 'struck': 25, 'that': 26, 'their': 27, 'these': 28, 'to': 29, 'was': 30, 'will': 31, \"''\": 32, 'dominated': 33, 'downsizers': 34, 'gospel': 35, 'hard-knock': 36, 'is': 37, 'minimalists': 38, 'or': 39, 'preach': 40, 'the': 41, 'those': 42, 'today': 43, 'travel': 44, 'wanderlust': 45, 'who': 46, '#': 47, '``': 48, 'admiration.they': 49, 'all': 50, 'almost': 51, 'always': 52, 'and': 53, 'are': 54, 'as': 55, 'aspire': 56, 'backpack': 57, 'characters': 58, 'for-instagram': 59, 'made-': 60, 'motorcyclediaries': 61, 'need': 62, 'reap': 63, 'sayings': 64, 'such': 65, 'they': 66, 'to.examples': 67, 'universal': 68, 'you': 69, 'gallivanting': 70, 'join': 71, 'philosophers': 72, 'unable': 73, 'at': 74, 'brave': 75, 'give': 76, 'marvel': 77, 'others': 78, 'overpriced': 79, 'predictability': 80, 'rebellion-oh.to': 81, 'sigh': 82, 'someday.they': 83, 'tourist': 84, 'traps': 85, 'up': 86, 'billionaire': 87, 'cocktail': 88, 'context': 89, 'evokes': 90, 'in': 91, 'luxury': 92, 'molotov': 93, 'on': 94, 'teelings': 95, 'this': 96}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKIzSpXwsqKx",
        "outputId": "d46a1d1e-c8ec-4857-f332-81f4df5acf5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# 3.txt\n",
        "dictionary3 = gensim.corpora.Dictionary(gen_docs3)\n",
        "print(dictionary3.token2id)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"''\": 0, ',': 1, '.': 2, '?': 3, 'a': 4, 'about': 5, 'abroad': 6, 'and': 7, 'baklava-a': 8, 'beeline': 9, 'bite': 10, 'bombay': 11, 'call': 12, 'centre': 13, 'decadence': 14, 'disagree-': 15, 'dry': 16, 'eatery': 17, 'every': 18, 'faithfully': 19, 'favourite': 20, 'feeling': 21, 'food.a': 22, 'for': 23, 'for.his': 24, 'friend': 25, 'fruit-laden': 26, 'guidebook-toting': 27, 'in': 28, 'kind': 29, 'like': 30, 'locals': 31, 'make': 32, 'memorable': 33, 'ments': 34, 'menu': 35, 'money': 36, 'my': 37, 'of': 38, 'often': 39, 'on': 40, 'overrated': 41, 'particularly': 42, 'restaurant': 43, 'settle': 44, 'smacked': 45, 'south': 46, 'storied': 47, 'sweet': 48, 'that': 49, 'the': 50, 'to': 51, 'tourists': 52, 'traditional': 53, 'was': 54, 'who': 55, 'wistful': 56, '“': 57, 'dessert': 58, 'first': 59, 'he': 60, 'requested': 61, 'time': 62}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEZiMvo6sy8Q",
        "outputId": "16670702-6ed8-462e-b31e-db87fec9d334",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# 4.txt\n",
        "dictionary4 = gensim.corpora.Dictionary(gen_docs4)\n",
        "print(dictionary4.token2id)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"''\": 0, ',': 1, '.': 2, 'a': 3, 'am': 4, 'and': 5, 'any': 6, 'before': 7, 'bit': 8, 'blame': 9, 'bombay': 10, 'choose': 11, 'city': 12, 'coming': 13, 'crooked': 14, 'descriptors': 15, 'dull.l': 16, 'elegance.this': 17, 'especially': 18, 'eternally': 19, 'fight.what': 20, 'for': 21, 'go': 22, 'growing': 23, 'hankering': 24, 'highways': 25, 'home': 26, 'i': 27, 'in': 28, 'india': 29, 'is': 30, 'like': 31, 'many': 32, 'mark': 33, 'matter': 34, 'mayhem': 35, 'me': 36, 'mean': 37, 'my': 38, 'never': 39, 'no': 40, 'of': 41, 'over': 42, 'place.let': 43, 'pow-wow': 44, 'predilections.one': 45, 'pristine': 46, 'pugilistic': 47, 'relation': 48, 'rephrase': 49, 'straight': 50, 'streets': 51, 'sweaty': 52, 'that': 53, 'the': 54, 'think': 55, 'this': 56, 'to': 57, 'twain': 58, 'up': 59, 'used': 60, 'was': 61, 'where': 62, 'why': 63, 'world': 64, 'would': 65, 'you': 66, ':': 67, '?': 68, '``': 69, 'again': 70, 'arabian': 71, 'bewildering': 72, 'bewitching': 73, 'come': 74, 'confound': 75, 'him': 76, 'nights': 77, 'place': 78, 'seemed': 79, '＂enchanting': 80, '-': 81, 'at': 82, 'befitting': 83, 'exclamations': 84, 'he': 85, 'man': 86, 'number': 87, 'repulsed': 88, 'riveted': 89, 'same': 90, 'time.it': 91}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4CQIE_4tcLb"
      },
      "source": [
        "# Making a bag of words\n",
        "### The next important object you need to familiarize with in order to work in gensim is the Corpus (a Bag of Words). It is a basically object that contains the word id and its frequency in each document (just lists the number of times each word occurs in the sentence).\n",
        "\n",
        "### Note that, a ‘token’ typically means a ‘word’. A ‘document’ can typically refer to a ‘sentence’ or ‘paragraph’ and a ‘corpus’ is typically a ‘collection of documents as a bag of words’.\n",
        "\n",
        "### Now, create a bag of words corpus and pass the tokenized list of words to the Dictionary.doc2bow()\n",
        "\n",
        "### (a,b) a=>id, b=>count in the document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ccWd6o3tsNq",
        "outputId": "bfaba6cf-6061-4cbf-ecc3-2d8f20d5983e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# 1.txt\n",
        "corpus1 = [dictionary1.doc2bow(gen_doc) for gen_doc in gen_docs1]\n",
        "print(corpus1)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 2), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 3), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1)], [(0, 1), (1, 2), (3, 1), (4, 1), (11, 1), (14, 1), (17, 1), (19, 2), (20, 1), (21, 1), (22, 1), (23, 4), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 2), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 2), (63, 1), (64, 1), (65, 1), (66, 1), (67, 3), (68, 1), (69, 1)], [(1, 2), (4, 2), (11, 1), (14, 1), (20, 1), (23, 1), (29, 1), (39, 2), (55, 1), (63, 2), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1)]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTcNesWCvb1t",
        "outputId": "1553c72b-39c4-4253-8ea2-d2e926a93631",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# 2.txt\n",
        "corpus2 = [dictionary2.doc2bow(gen_doc) for gen_doc in gen_docs2]\n",
        "print(corpus2)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)], [(0, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1)], [(0, 1), (9, 1), (12, 1), (21, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1)], [(0, 1), (9, 1), (10, 1), (21, 1), (29, 1), (32, 4), (37, 1), (39, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1)], [(0, 1), (28, 1), (29, 1), (70, 1), (71, 1), (72, 1), (73, 1)], [(0, 1), (21, 1), (27, 1), (41, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1)], [(9, 1), (10, 2), (21, 1), (44, 1), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1)]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD22DZmNvjeE",
        "outputId": "8aba6fb7-0d60-4fbc-8fa5-5141e67cb281",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# 3.txt\n",
        "corpus3 = [dictionary3.doc2bow(gen_doc) for gen_doc in gen_docs3]\n",
        "print(corpus3)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[(0, 1), (1, 2), (2, 1), (3, 1), (4, 2), (5, 2), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 2), (39, 1), (40, 2), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 2), (50, 3), (51, 2), (52, 1), (53, 1), (54, 2), (55, 1), (56, 1), (57, 1)], [(23, 1), (50, 2), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1)]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ti6xI_e7wt1Q",
        "outputId": "1075c133-a8c7-4d95-b4c4-3f55d64745a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# 4.txt\n",
        "corpus4 = [dictionary4.doc2bow(gen_doc) for gen_doc in gen_docs4]\n",
        "print(corpus4)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[(0, 1), (1, 4), (2, 1), (3, 2), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 2), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 2), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 5), (28, 4), (29, 1), (30, 3), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 2), (42, 2), (43, 1), (44, 2), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 2), (55, 1), (56, 1), (57, 2), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1)], [(0, 5), (1, 2), (54, 1), (57, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1)], [(2, 1), (3, 1), (5, 1), (41, 1), (54, 3), (60, 1), (61, 2), (69, 1), (78, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1)]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TU955-lvJUW"
      },
      "source": [
        "# TFIDF\n",
        "##### Term Frequency – Inverse Document Frequency(TF-IDF) is also a bag-of-words model but unlike the regular corpus, TFIDF down weights tokens (words) that appears frequently across documents.\n",
        "\n",
        "##### Tf-Idf is calculated by multiplying a local component (TF) with a global component (IDF) and optionally normalizing the result to unit length. Term frequency is how often the word shows up in the document and inverse document frequency scales the value by how rare the word is in the corpus. In simple terms, words that occur more frequently across the documents get smaller weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvGEu0nLvSkD",
        "outputId": "f777aae8-0941-4196-d669-843a0a0a85ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "# 1.txt\n",
        "import numpy as np\n",
        "tfidf1 = gensim.models.TfidfModel(corpus1)\n",
        "print(corpus1)\n",
        "print(\"\")\n",
        "for doc in tfidf1[corpus1]:\n",
        "    print([[dictionary1[id], np.around(freq, decimals=2)] for id, freq in doc])"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 2), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 3), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1)], [(0, 1), (1, 2), (3, 1), (4, 1), (11, 1), (14, 1), (17, 1), (19, 2), (20, 1), (21, 1), (22, 1), (23, 4), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 2), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 2), (63, 1), (64, 1), (65, 1), (66, 1), (67, 3), (68, 1), (69, 1)], [(1, 2), (4, 2), (11, 1), (14, 1), (20, 1), (23, 1), (29, 1), (39, 2), (55, 1), (63, 2), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1)]]\n",
            "\n",
            "[['(', 0.1], ['.', 0.28], [':', 0.1], ['anniversary', 0.28], ['be', 0.28], ['editions', 0.28], ['feel', 0.28], ['graduation', 0.28], ['have', 0.28], ['slogging', 0.28], ['studious', 0.28], ['told', 0.28], ['truth', 0.28], ['which', 0.1], ['year', 0.28]]\n",
            "[['(', 0.05], [':', 0.05], ['which', 0.05], [')', 0.25], ['?', 0.05], ['account', 0.12], ['achievement', 0.12], ['and', 0.18], ['another', 0.12], [\"anxiety.there's\", 0.12], ['could', 0.12], ['day', 0.12], ['did', 0.12], ['do', 0.05], ['for', 0.12], ['fun', 0.12], ['geographic', 0.12], ['get', 0.12], ['gnawing', 0.12], ['has', 0.12], ['in', 0.25], ['india', 0.12], ['indulge', 0.12], ['it', 0.05], ['l', 0.12], ['landscape', 0.12], ['lingering', 0.12], ['little', 0.12], ['lived', 0.12], ['madcap', 0.12], ['media', 0.12], ['more', 0.12], ['my', 0.12], ['national', 0.12], ['off', 0.12], ['only', 0.12], ['precarious', 0.12], ['pride', 0.12], ['question', 0.12], ['right', 0.05], ['rounded', 0.12], ['see', 0.12], ['sense', 0.12], ['should', 0.12], ['something.then', 0.12], ['team', 0.12], ['that', 0.25], ['to', 0.05], [\"today's\", 0.12], ['traveller', 0.12], ['very', 0.12], ['we', 0.37], ['wish', 0.12], ['with', 0.12]]\n",
            "[['?', 0.07], ['and', 0.07], ['do', 0.07], ['it', 0.15], ['right', 0.07], ['to', 0.15], [\"'s\", 0.2], ['albert', 0.2], ['around', 0.2], ['asia', 0.2], ['comes', 0.2], ['dliscovered', 0.2], ['early', 0.2], ['einstein', 0.2], ['entries', 0.2], ['his', 0.2], ['is', 0.2], ['journeys', 0.2], ['month', 0.2], ['new', 0.2], ['or', 0.2], ['there', 0.2], ['this', 0.2], ['times', 0.2], ['travel', 0.2], ['unearthed', 0.2], ['way', 0.2], ['when', 0.2], ['wrong', 0.2], ['york', 0.2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9333SgkX4OPC",
        "outputId": "39236c98-b21a-4da7-fb36-ebe55a257583",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "# 2.txt\n",
        "import numpy as np\n",
        "tfidf2 = gensim.models.TfidfModel(corpus2)\n",
        "print(corpus2)\n",
        "print(\"\")\n",
        "for doc in tfidf2[corpus2]:\n",
        "    print([[dictionary2[id], np.around(freq, decimals=2)] for id, freq in doc])"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)], [(0, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1)], [(0, 1), (9, 1), (12, 1), (21, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1)], [(0, 1), (9, 1), (10, 1), (21, 1), (29, 1), (32, 4), (37, 1), (39, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1)], [(0, 1), (28, 1), (29, 1), (70, 1), (71, 1), (72, 1), (73, 1)], [(0, 1), (21, 1), (27, 1), (41, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1)], [(9, 1), (10, 2), (21, 1), (44, 1), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1)]]\n",
            "\n",
            "[['.', 0.03], ['edition', 0.35], ['indulgence', 0.35], ['our', 0.35], ['toasts', 0.35], ['travelling', 0.35], ['ultra-', 0.35], ['while', 0.35], ['year-end', 0.35]]\n",
            "[['.', 0.02], [',', 0.07], ['a', 0.1], ['be', 0.24], ['by', 0.15], ['contrast', 0.24], ['featuring', 0.24], ['financial', 0.24], ['i', 0.24], ['itineraries', 0.24], ['know', 0.24], ['many', 0.24], ['narratives', 0.24], ['of', 0.04], ['out', 0.24], ['producing', 0.24], ['reach.in', 0.24], ['struck', 0.24], ['that', 0.24], ['their', 0.15], ['these', 0.15], ['to', 0.1], ['was', 0.24], ['will', 0.24]]\n",
            "[['.', 0.02], [',', 0.08], ['by', 0.18], ['of', 0.05], [\"''\", 0.18], ['dominated', 0.28], ['downsizers', 0.28], ['gospel', 0.28], ['hard-knock', 0.28], ['is', 0.18], ['minimalists', 0.28], ['or', 0.18], ['preach', 0.28], ['the', 0.18], ['those', 0.28], ['today', 0.28], ['travel', 0.18], ['wanderlust', 0.28], ['who', 0.28]]\n",
            "[['.', 0.01], [',', 0.05], ['a', 0.08], ['of', 0.03], ['to', 0.08], [\"''\", 0.46], ['is', 0.12], ['or', 0.12], ['#', 0.18], ['``', 0.18], ['admiration.they', 0.18], ['all', 0.18], ['almost', 0.18], ['always', 0.18], ['and', 0.18], ['are', 0.18], ['as', 0.18], ['aspire', 0.18], ['backpack', 0.18], ['characters', 0.18], ['for-instagram', 0.18], ['made-', 0.18], ['motorcyclediaries', 0.18], ['need', 0.18], ['reap', 0.18], ['sayings', 0.18], ['such', 0.18], ['they', 0.18], ['to.examples', 0.18], ['universal', 0.18], ['you', 0.18]]\n",
            "[['.', 0.04], ['these', 0.3], ['to', 0.2], ['gallivanting', 0.47], ['join', 0.47], ['philosophers', 0.47], ['unable', 0.47]]\n",
            "[['.', 0.02], ['of', 0.05], ['their', 0.17], ['the', 0.17], ['at', 0.27], ['brave', 0.27], ['give', 0.27], ['marvel', 0.27], ['others', 0.27], ['overpriced', 0.27], ['predictability', 0.27], ['rebellion-oh.to', 0.27], ['sigh', 0.27], ['someday.they', 0.27], ['tourist', 0.27], ['traps', 0.27], ['up', 0.27]]\n",
            "[[',', 0.09], ['a', 0.26], ['of', 0.05], ['travel', 0.19], ['billionaire', 0.3], ['cocktail', 0.3], ['context', 0.3], ['evokes', 0.3], ['in', 0.3], ['luxury', 0.3], ['molotov', 0.3], ['on', 0.3], ['teelings', 0.3], ['this', 0.3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGp6RZBf4nhM",
        "outputId": "3e228739-cab2-487f-b6e2-c419ee793c45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "# 3.txt\n",
        "import numpy as np\n",
        "tfidf3 = gensim.models.TfidfModel(corpus3)\n",
        "print(corpus3)\n",
        "print(\"\")\n",
        "for doc in tfidf3[corpus3]:\n",
        "    print([[dictionary3[id], np.around(freq, decimals=2)] for id, freq in doc])"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[(0, 1), (1, 2), (2, 1), (3, 1), (4, 2), (5, 2), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 2), (39, 1), (40, 2), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 2), (50, 3), (51, 2), (52, 1), (53, 1), (54, 2), (55, 1), (56, 1), (57, 1)], [(23, 1), (50, 2), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1)]]\n",
            "\n",
            "[[\"''\", 0.11], [',', 0.22], ['.', 0.11], ['?', 0.11], ['a', 0.22], ['about', 0.22], ['abroad', 0.11], ['and', 0.11], ['baklava-a', 0.11], ['beeline', 0.11], ['bite', 0.11], ['bombay', 0.11], ['call', 0.11], ['centre', 0.11], ['decadence', 0.11], ['disagree-', 0.11], ['dry', 0.11], ['eatery', 0.11], ['every', 0.11], ['faithfully', 0.11], ['favourite', 0.11], ['feeling', 0.11], ['food.a', 0.11], ['for.his', 0.11], ['friend', 0.11], ['fruit-laden', 0.11], ['guidebook-toting', 0.11], ['in', 0.11], ['kind', 0.11], ['like', 0.11], ['locals', 0.11], ['make', 0.11], ['memorable', 0.11], ['ments', 0.11], ['menu', 0.11], ['money', 0.11], ['my', 0.11], ['of', 0.22], ['often', 0.11], ['on', 0.22], ['overrated', 0.11], ['particularly', 0.11], ['restaurant', 0.11], ['settle', 0.11], ['smacked', 0.11], ['south', 0.11], ['storied', 0.11], ['sweet', 0.11], ['that', 0.22], ['to', 0.22], ['tourists', 0.11], ['traditional', 0.11], ['was', 0.22], ['who', 0.11], ['wistful', 0.11], ['“', 0.11]]\n",
            "[['dessert', 0.45], ['first', 0.45], ['he', 0.45], ['requested', 0.45], ['time', 0.45]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5lGjsgL5u15",
        "outputId": "0ac4bb08-be2a-4364-d097-3ad24baf26b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "# 4.txt\n",
        "import numpy as np\n",
        "tfidf4 = gensim.models.TfidfModel(corpus4)\n",
        "print(corpus4)\n",
        "print(\"\")\n",
        "for doc in tfidf4[corpus4]:\n",
        "    print([[dictionary4[id], np.around(freq, decimals=2)] for id, freq in doc])"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[(0, 1), (1, 4), (2, 1), (3, 2), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 2), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 2), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 5), (28, 4), (29, 1), (30, 3), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 2), (42, 2), (43, 1), (44, 2), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 2), (55, 1), (56, 1), (57, 2), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1)], [(0, 5), (1, 2), (54, 1), (57, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1)], [(2, 1), (3, 1), (5, 1), (41, 1), (54, 3), (60, 1), (61, 2), (69, 1), (78, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1)]]\n",
            "\n",
            "[[\"''\", 0.03], [',', 0.13], ['.', 0.03], ['a', 0.07], ['am', 0.09], ['and', 0.03], ['any', 0.09], ['before', 0.09], ['bit', 0.09], ['blame', 0.09], ['bombay', 0.18], ['choose', 0.09], ['city', 0.09], ['coming', 0.09], ['crooked', 0.09], ['descriptors', 0.09], ['dull.l', 0.09], ['elegance.this', 0.09], ['especially', 0.09], ['eternally', 0.09], ['fight.what', 0.09], ['for', 0.18], ['go', 0.09], ['growing', 0.09], ['hankering', 0.09], ['highways', 0.09], ['home', 0.09], ['i', 0.46], ['in', 0.36], ['india', 0.09], ['is', 0.27], ['like', 0.09], ['many', 0.09], ['mark', 0.09], ['matter', 0.09], ['mayhem', 0.09], ['me', 0.09], ['mean', 0.09], ['my', 0.09], ['never', 0.09], ['no', 0.09], ['of', 0.07], ['over', 0.18], ['place.let', 0.09], ['pow-wow', 0.18], ['predilections.one', 0.09], ['pristine', 0.09], ['pugilistic', 0.09], ['relation', 0.09], ['rephrase', 0.09], ['straight', 0.09], ['streets', 0.09], ['sweaty', 0.09], ['that', 0.09], ['think', 0.09], ['this', 0.09], ['to', 0.07], ['twain', 0.09], ['up', 0.09], ['used', 0.03], ['was', 0.03], ['where', 0.09], ['why', 0.09], ['world', 0.09], ['would', 0.09], ['you', 0.09]]\n",
            "[[\"''\", 0.46], [',', 0.18], ['to', 0.09], [':', 0.25], ['?', 0.25], ['``', 0.09], ['again', 0.25], ['arabian', 0.25], ['bewildering', 0.25], ['bewitching', 0.25], ['come', 0.25], ['confound', 0.25], ['him', 0.25], ['nights', 0.25], ['place', 0.09], ['seemed', 0.25], ['＂enchanting', 0.25]]\n",
            "[['.', 0.1], ['a', 0.1], ['and', 0.1], ['of', 0.1], ['used', 0.1], ['was', 0.21], ['``', 0.1], ['place', 0.1], ['-', 0.28], ['at', 0.28], ['befitting', 0.28], ['exclamations', 0.28], ['he', 0.28], ['man', 0.28], ['number', 0.28], ['repulsed', 0.28], ['riveted', 0.28], ['same', 0.28], ['time.it', 0.28]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBEUPDOH6Vyv"
      },
      "source": [
        "#Creating similarity measure object\n",
        "##### Now, we are going to create similarity object. The main class is Similarity, which builds an index for a given set of documents.The Similarity class splits the index into several smaller sub-indexes, which are disk-based. Let's just create similarity object then you will understand how we can use it for comparing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XULPqPp_6_TH",
        "outputId": "c540285d-4370-45a5-a9d9-a8aadca6e291",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "# building the index\n",
        "sims1 = gensim.similarities.Similarity('workdir1/',tfidf1[corpus1],\n",
        "                                        num_features=len(dictionary1))"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:718: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5vy_nQZ_-Ey",
        "outputId": "ed65883a-44d3-487b-b4e6-a4c9338e0989",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "# building the index\n",
        "sims1 = gensim.similarities.Similarity('/content/drive',tfidf1[corpus1],\n",
        "                                        num_features=len(dictionary1))"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:718: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLxQxDIFCY0I",
        "outputId": "f415949e-6a02-4c56-a303-d51f54dd9a0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "file2_docs = []\n",
        "\n",
        "with open ('2.txt') as f:\n",
        "    tokens = sent_tokenize(f.read())\n",
        "    for line in tokens:\n",
        "        file2_docs.append(line)\n",
        "# with open ('3.txt') as f:\n",
        "#     tokens = sent_tokenize(f.read())\n",
        "#     for line in tokens:\n",
        "#         file2_docs.append(line)\n",
        "# with open ('3.txt') as f:\n",
        "#     tokens = sent_tokenize(f.read())\n",
        "#     for line in tokens:\n",
        "#         file2_docs.append(line)\n",
        "\n",
        "print(\"Number of documents:\",len(file2_docs))  \n",
        "for line in file2_docs:\n",
        "    query_doc = [w.lower() for w in word_tokenize(line)]\n",
        "    query_doc_bow = dictionary1.doc2bow(query_doc) \n",
        "    #update an existing dictionary and create bag of words"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of documents: 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPUsqcX2DZGq"
      },
      "source": [
        "# Document similarities to query\n",
        "At this stage, you will see similarities between the query and all index documents. To obtain similarities of our query document against the indexed documents:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMF_qUczDbbj",
        "outputId": "c9f92641-7dc7-488f-fd1a-2a4a12959bd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# perform a similarity query against the corpus\n",
        "query_doc_tf_idf = tfidf1[query_doc_bow]\n",
        "# print(document_number, document_similarity)\n",
        "print('Comparing Result:', sims1[query_doc_tf_idf]) "
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Comparing Result: [0.         0.14292741 0.22806387]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/similarities/docsim.py:518: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  result = numpy.hstack(shard_results)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvj9AjaUGg1O"
      },
      "source": [
        "# Cosine measure returns similarities in the range (the greater, the more similar)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyuyO936Grro"
      },
      "source": [
        "#Calculating average similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92majOMnGwFJ",
        "outputId": "d55c8a3c-95ba-4418-9655-7f4bf7b9d212",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "sum_of_sims =(np.sum(sims1[query_doc_tf_idf], dtype=np.float32))\n",
        "print(sum_of_sims)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3709913\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/similarities/docsim.py:518: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  result = numpy.hstack(shard_results)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3EmYCn_Hj1g",
        "outputId": "e9e9f334-34f8-43f2-a8c0-270cde8d53c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "percentage_of_similarity = round(float((sum_of_sims / len(file_docs)) * 100))\n",
        "print(f'Average similarity float: {float(sum_of_sims / len(file_docs))}')\n",
        "print(f'Average similarity percentage: {float(sum_of_sims / len(file_docs)) * 100}')\n",
        "print(f'Average similarity rounded percentage: {percentage_of_similarity}')"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average similarity float: 0.04122125440173679\n",
            "Average similarity percentage: 4.122125440173678\n",
            "Average similarity rounded percentage: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEVRLekoItDC"
      },
      "source": [
        "# Here we conclude that query document 1 and 2 are approximately 4.1% similar. Similarly we compare 1.txt with 2.txt, 3.txt and 4.txt respectively: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO0J2gy3IH9L",
        "outputId": "5a41d1db-f59f-4a17-f2d3-f663df39429c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "print(\"For 3.txt\")\n",
        "file2_docs = []\n",
        "\n",
        "with open ('3.txt') as f:\n",
        "    tokens = sent_tokenize(f.read())\n",
        "    for line in tokens:\n",
        "        file2_docs.append(line)\n",
        "\n",
        "print(\"Number of documents:\",len(file2_docs))  \n",
        "for line in file2_docs:\n",
        "    query_doc = [w.lower() for w in word_tokenize(line)]\n",
        "    query_doc_bow = dictionary1.doc2bow(query_doc) \n",
        "    #update an existing dictionary and create bag of words\n",
        "# perform a similarity query against the corpus\n",
        "query_doc_tf_idf = tfidf1[query_doc_bow]\n",
        "# print(document_number, document_similarity)\n",
        "print('Comparing Result:', sims1[query_doc_tf_idf]) \n",
        "import numpy as np\n",
        "\n",
        "sum_of_sims =(np.sum(sims1[query_doc_tf_idf], dtype=np.float32))\n",
        "print(sum_of_sims)\n",
        "percentage_of_similarity = round(float((sum_of_sims / len(file_docs)) * 100))\n",
        "print(f'Average similarity float: {float(sum_of_sims / len(file_docs))}')\n",
        "print(f'Average similarity percentage: {float(sum_of_sims / len(file_docs)) * 100}')\n",
        "print(f'Average similarity rounded percentage: {percentage_of_similarity}')\n",
        "print(\"================================================\")\n",
        "print(\"================================================\")\n",
        "print(\"================================================\")\n",
        "\n",
        "\n",
        "print(\"For 4.txt\")\n",
        "file2_docs = []\n",
        "\n",
        "with open ('4.txt') as f:\n",
        "    tokens = sent_tokenize(f.read())\n",
        "    for line in tokens:\n",
        "        file2_docs.append(line)\n",
        "# with open ('3.txt') as f:\n",
        "#     tokens = sent_tokenize(f.read())\n",
        "#     for line in tokens:\n",
        "#         file2_docs.append(line)\n",
        "# with open ('3.txt') as f:\n",
        "#     tokens = sent_tokenize(f.read())\n",
        "#     for line in tokens:\n",
        "#         file2_docs.append(line)\n",
        "\n",
        "print(\"Number of documents:\",len(file2_docs))  \n",
        "for line in file2_docs:\n",
        "    query_doc = [w.lower() for w in word_tokenize(line)]\n",
        "    query_doc_bow = dictionary1.doc2bow(query_doc) \n",
        "    #update an existing dictionary and create bag of words\n",
        "# perform a similarity query against the corpus\n",
        "query_doc_tf_idf = tfidf1[query_doc_bow]\n",
        "# print(document_number, document_similarity)\n",
        "print('Comparing Result:', sims1[query_doc_tf_idf]) \n",
        "import numpy as np\n",
        "\n",
        "sum_of_sims =(np.sum(sims1[query_doc_tf_idf], dtype=np.float32))\n",
        "print(sum_of_sims)\n",
        "percentage_of_similarity = round(float((sum_of_sims / len(file_docs)) * 100))\n",
        "print(f'Average similarity float: {float(sum_of_sims / len(file_docs))}')\n",
        "print(f'Average similarity percentage: {float(sum_of_sims / len(file_docs)) * 100}')\n",
        "print(f'Average similarity rounded percentage: {percentage_of_similarity}')\n",
        "print(\"================================================\")\n",
        "print(\"================================================\")\n",
        "print(\"================================================\")"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For 3.txt\n",
            "Number of documents: 2\n",
            "Comparing Result: [0.         0.12377877 0.        ]\n",
            "0.12377877\n",
            "Average similarity float: 0.013753196431530846\n",
            "Average similarity percentage: 1.3753196431530845\n",
            "Average similarity rounded percentage: 1\n",
            "================================================\n",
            "================================================\n",
            "================================================\n",
            "For 4.txt\n",
            "Number of documents: 3\n",
            "Comparing Result: [0.26632264 0.0632695  0.02523918]\n",
            "0.35483134\n",
            "Average similarity float: 0.039425704214307994\n",
            "Average similarity percentage: 3.9425704214307995\n",
            "Average similarity rounded percentage: 4\n",
            "================================================\n",
            "================================================\n",
            "================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/similarities/docsim.py:518: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  result = numpy.hstack(shard_results)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORYqeTncLhYT"
      },
      "source": [
        "#Here we conclude that documents 1.txt and 3.txt are 1%, and 1.txt and 4.txt are 4% similar. Please refer to the similarity float value mentioned above. Here. we are using 1.txt as query document and the other documents for comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfYKEMpCMR4_"
      },
      "source": [
        "# =============================================================================================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5FDtHoXO6yz"
      },
      "source": [
        "#Now we clubbed 2.txt, 3.txt and 4.txt together and compared it with 1.txt to find average similarity index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62AJx_G0MPdF",
        "outputId": "a97270d7-3d25-4421-9fa8-40b671a142c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "avg_sims = [] # array of averages\n",
        "with open ('4.txt') as f:\n",
        "    tokens = sent_tokenize(f.read())\n",
        "    for line in tokens:\n",
        "        file2_docs.append(line)\n",
        "with open ('2.txt') as f:\n",
        "    tokens = sent_tokenize(f.read())\n",
        "    for line in tokens:\n",
        "        file2_docs.append(line)\n",
        "with open ('3.txt') as f:\n",
        "    tokens = sent_tokenize(f.read())\n",
        "    for line in tokens:\n",
        "        file2_docs.append(line)\n",
        "\n",
        "# for line in query documents\n",
        "for line in file2_docs:\n",
        "        # tokenize words\n",
        "        query_doc = [w.lower() for w in word_tokenize(line)]\n",
        "        # create bag of words\n",
        "        query_doc_bow = dictionary1.doc2bow(query_doc)\n",
        "        # find similarity for each document\n",
        "        query_doc_tf_idf = tf_idf[query_doc_bow]\n",
        "        # print (document_number, document_similarity)\n",
        "        print('Comparing Result:', sims1[query_doc_tf_idf]) \n",
        "        # calculate sum of similarities for each query doc\n",
        "        sum_of_sims =(np.sum(sims1[query_doc_tf_idf], dtype=np.float32))\n",
        "        # calculate average of similarity for each query doc\n",
        "        avg = sum_of_sims / len(file_docs)\n",
        "        # print average of similarity for each query doc\n",
        "        print(f'avg: {sum_of_sims / len(file_docs)}')\n",
        "        # add average values into array\n",
        "        avg_sims.append(avg)  \n",
        "   # calculate total average\n",
        "total_avg = np.sum(avg_sims, dtype=np.float)/len(file2_docs)\n",
        "    # round the value and multiply by 100 to format it as percentage\n",
        "print(total_avg)\n",
        "percentage_of_similarity = round(float(total_avg) * 100)\n",
        "print(\"===\")\n",
        "print(\"===\")\n",
        "print(\"===\")\n",
        "\n",
        "print(\"The perecntage of similarity is therefore in %:\", percentage_of_similarity)\n",
        "    # if percentage is greater than 100\n",
        "    # that means documents are almost same\n",
        "# if percentage_of_similarity >= 100:\n",
        "#     percentage_of_similarity = 100\n",
        "# print(percentage_of_similarity)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/similarities/docsim.py:518: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  result = numpy.hstack(shard_results)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Comparing Result: [0.04820496 0.3114357  0.1569949 ]\n",
            "avg: 0.05740394857194689\n",
            "Comparing Result: [0.06049038 0.07912537 0.12625737]\n",
            "avg: 0.02954145934846666\n",
            "Comparing Result: [0.26632264 0.0632695  0.02523918]\n",
            "avg: 0.039425704214307994\n",
            "Comparing Result: [0.04820496 0.3114357  0.1569949 ]\n",
            "avg: 0.05740394857194689\n",
            "Comparing Result: [0.06049038 0.07912537 0.12625737]\n",
            "avg: 0.02954145934846666\n",
            "Comparing Result: [0.26632264 0.0632695  0.02523918]\n",
            "avg: 0.039425704214307994\n",
            "Comparing Result: [0.09560282 0.37322545 0.05436122]\n",
            "avg: 0.058132165008121066\n",
            "Comparing Result: [0.         0.12377877 0.        ]\n",
            "avg: 0.013753196431530846\n",
            "Comparing Result: [0.09560282 0.37322545 0.05436122]\n",
            "avg: 0.058132165008121066\n",
            "Comparing Result: [0.         0.12377877 0.        ]\n",
            "avg: 0.013753196431530846\n",
            "Comparing Result: [0.04820496 0.3114357  0.1569949 ]\n",
            "avg: 0.05740394857194689\n",
            "Comparing Result: [0.06049038 0.07912537 0.12625737]\n",
            "avg: 0.02954145934846666\n",
            "Comparing Result: [0.26632264 0.0632695  0.02523918]\n",
            "avg: 0.039425704214307994\n",
            "Comparing Result: [0.09560282 0.37322545 0.05436122]\n",
            "avg: 0.058132165008121066\n",
            "Comparing Result: [0.         0.12377877 0.        ]\n",
            "avg: 0.013753196431530846\n",
            "Comparing Result: [0.09560282 0.37322545 0.05436122]\n",
            "avg: 0.058132165008121066\n",
            "Comparing Result: [0.         0.12377877 0.        ]\n",
            "avg: 0.013753196431530846\n",
            "Comparing Result: [0.04820496 0.3114357  0.1569949 ]\n",
            "avg: 0.05740394857194689\n",
            "Comparing Result: [0.06049038 0.07912537 0.12625737]\n",
            "avg: 0.02954145934846666\n",
            "Comparing Result: [0.26632264 0.0632695  0.02523918]\n",
            "avg: 0.039425704214307994\n",
            "Comparing Result: [0.09560282 0.37322545 0.05436122]\n",
            "avg: 0.058132165008121066\n",
            "Comparing Result: [0.         0.12377877 0.        ]\n",
            "avg: 0.013753196431530846\n",
            "Comparing Result: [0.09560282 0.37322545 0.05436122]\n",
            "avg: 0.058132165008121066\n",
            "Comparing Result: [0.         0.12377877 0.        ]\n",
            "avg: 0.013753196431530846\n",
            "Comparing Result: [0.04820496 0.3114357  0.1569949 ]\n",
            "avg: 0.05740394857194689\n",
            "Comparing Result: [0.06049038 0.07912537 0.12625737]\n",
            "avg: 0.02954145934846666\n",
            "Comparing Result: [0.26632264 0.0632695  0.02523918]\n",
            "avg: 0.039425704214307994\n",
            "Comparing Result: [0.09560282 0.37322545 0.05436122]\n",
            "avg: 0.058132165008121066\n",
            "Comparing Result: [0.         0.12377877 0.        ]\n",
            "avg: 0.013753196431530846\n",
            "Comparing Result: [0.09560282 0.37322545 0.05436122]\n",
            "avg: 0.058132165008121066\n",
            "Comparing Result: [0.         0.12377877 0.        ]\n",
            "avg: 0.013753196431530846\n",
            "Comparing Result: [0.04820496 0.3114357  0.1569949 ]\n",
            "avg: 0.05740394857194689\n",
            "Comparing Result: [0.06049038 0.07912537 0.12625737]\n",
            "avg: 0.02954145934846666\n",
            "Comparing Result: [0.26632264 0.0632695  0.02523918]\n",
            "avg: 0.039425704214307994\n",
            "Comparing Result: [0.09560282 0.37322545 0.05436122]\n",
            "avg: 0.058132165008121066\n",
            "Comparing Result: [0.         0.12377877 0.        ]\n",
            "avg: 0.013753196431530846\n",
            "Comparing Result: [0.09560282 0.37322545 0.05436122]\n",
            "avg: 0.058132165008121066\n",
            "Comparing Result: [0.         0.12377877 0.        ]\n",
            "avg: 0.013753196431530846\n",
            "Comparing Result: [0.04820496 0.3114357  0.1569949 ]\n",
            "avg: 0.05740394857194689\n",
            "Comparing Result: [0.06049038 0.07912537 0.12625737]\n",
            "avg: 0.02954145934846666\n",
            "Comparing Result: [0.26632264 0.0632695  0.02523918]\n",
            "avg: 0.039425704214307994\n",
            "Comparing Result: [0.09560282 0.37322545 0.05436122]\n",
            "avg: 0.058132165008121066\n",
            "Comparing Result: [0.         0.12377877 0.        ]\n",
            "avg: 0.013753196431530846\n",
            "Comparing Result: [0.09560282 0.37322545 0.05436122]\n",
            "avg: 0.058132165008121066\n",
            "Comparing Result: [0.         0.12377877 0.        ]\n",
            "avg: 0.013753196431530846\n",
            "Comparing Result: [0.04820496 0.3114357  0.1569949 ]\n",
            "avg: 0.05740394857194689\n",
            "Comparing Result: [0.06049038 0.07912537 0.12625737]\n",
            "avg: 0.02954145934846666\n",
            "Comparing Result: [0.26632264 0.0632695  0.02523918]\n",
            "avg: 0.039425704214307994\n",
            "Comparing Result: [0.09560282 0.37322545 0.05436122]\n",
            "avg: 0.058132165008121066\n",
            "Comparing Result: [0.         0.12377877 0.        ]\n",
            "avg: 0.013753196431530846\n",
            "Comparing Result: [0.09560282 0.37322545 0.05436122]\n",
            "avg: 0.058132165008121066\n",
            "Comparing Result: [0.         0.12377877 0.        ]\n",
            "avg: 0.013753196431530846\n",
            "Comparing Result: [0.04820496 0.3114357  0.1569949 ]\n",
            "avg: 0.05740394857194689\n",
            "Comparing Result: [0.06049038 0.07912537 0.12625737]\n",
            "avg: 0.02954145934846666\n",
            "Comparing Result: [0.26632264 0.0632695  0.02523918]\n",
            "avg: 0.039425704214307994\n",
            "Comparing Result: [0.09560282 0.37322545 0.05436122]\n",
            "avg: 0.058132165008121066\n",
            "Comparing Result: [0.         0.12377877 0.        ]\n",
            "avg: 0.013753196431530846\n",
            "Comparing Result: [0.09560282 0.37322545 0.05436122]\n",
            "avg: 0.058132165008121066\n",
            "Comparing Result: [0.         0.12377877 0.        ]\n",
            "avg: 0.013753196431530846\n",
            "Comparing Result: [0.04820496 0.3114357  0.1569949 ]\n",
            "avg: 0.05740394857194689\n",
            "Comparing Result: [0.06049038 0.07912537 0.12625737]\n",
            "avg: 0.02954145934846666\n",
            "Comparing Result: [0.26632264 0.0632695  0.02523918]\n",
            "avg: 0.039425704214307994\n",
            "Comparing Result: [0.09560282 0.37322545 0.05436122]\n",
            "avg: 0.058132165008121066\n",
            "Comparing Result: [0.         0.12377877 0.        ]\n",
            "avg: 0.013753196431530846\n",
            "Comparing Result: [0.09560282 0.37322545 0.05436122]\n",
            "avg: 0.058132165008121066\n",
            "Comparing Result: [0.         0.12377877 0.        ]\n",
            "avg: 0.013753196431530846\n",
            "Comparing Result: [0.04820496 0.3114357  0.1569949 ]\n",
            "avg: 0.05740394857194689\n",
            "Comparing Result: [0.06049038 0.07912537 0.12625737]\n",
            "avg: 0.02954145934846666\n",
            "Comparing Result: [0.26632264 0.0632695  0.02523918]\n",
            "avg: 0.039425704214307994\n",
            "Comparing Result: [0.28388205 0.         0.        ]\n",
            "avg: 0.031542450189590454\n",
            "Comparing Result: [0.32060122 0.14930966 0.03038312]\n",
            "avg: 0.055588225523630776\n",
            "Comparing Result: [0.14194103 0.         0.29626366]\n",
            "avg: 0.04868941174613105\n",
            "Comparing Result: [0.15692893 0.04660144 0.26298067]\n",
            "avg: 0.051834560102886625\n",
            "Comparing Result: [0.26632264 0.01581737 0.05047837]\n",
            "avg: 0.03695759839481778\n",
            "Comparing Result: [0.28388205 0.         0.        ]\n",
            "avg: 0.031542450189590454\n",
            "Comparing Result: [0.         0.14292741 0.22806387]\n",
            "avg: 0.04122125440173679\n",
            "Comparing Result: [0.09560282 0.37322545 0.05436122]\n",
            "avg: 0.058132165008121066\n",
            "Comparing Result: [0.         0.12377877 0.        ]\n",
            "avg: 0.013753196431530846\n",
            "Comparing Result: [0.04820496 0.3114357  0.1569949 ]\n",
            "avg: 0.05740394857194689\n",
            "Comparing Result: [0.06049038 0.07912537 0.12625737]\n",
            "avg: 0.02954145934846666\n",
            "Comparing Result: [0.26632264 0.0632695  0.02523918]\n",
            "avg: 0.039425704214307994\n",
            "Comparing Result: [0.28388205 0.         0.        ]\n",
            "avg: 0.031542450189590454\n",
            "Comparing Result: [0.32060122 0.14930966 0.03038312]\n",
            "avg: 0.055588225523630776\n",
            "Comparing Result: [0.14194103 0.         0.29626366]\n",
            "avg: 0.04868941174613105\n",
            "Comparing Result: [0.15692893 0.04660144 0.26298067]\n",
            "avg: 0.051834560102886625\n",
            "Comparing Result: [0.26632264 0.01581737 0.05047837]\n",
            "avg: 0.03695759839481778\n",
            "Comparing Result: [0.28388205 0.         0.        ]\n",
            "avg: 0.031542450189590454\n",
            "Comparing Result: [0.         0.14292741 0.22806387]\n",
            "avg: 0.04122125440173679\n",
            "Comparing Result: [0.09560282 0.37322545 0.05436122]\n",
            "avg: 0.058132165008121066\n",
            "Comparing Result: [0.         0.12377877 0.        ]\n",
            "avg: 0.013753196431530846\n",
            "Comparing Result: [0.04820496 0.3114357  0.1569949 ]\n",
            "avg: 0.05740394857194689\n",
            "Comparing Result: [0.06049038 0.07912537 0.12625737]\n",
            "avg: 0.02954145934846666\n",
            "Comparing Result: [0.26632264 0.0632695  0.02523918]\n",
            "avg: 0.039425704214307994\n",
            "Comparing Result: [0.28388205 0.         0.        ]\n",
            "avg: 0.031542450189590454\n",
            "Comparing Result: [0.32060122 0.14930966 0.03038312]\n",
            "avg: 0.055588225523630776\n",
            "Comparing Result: [0.14194103 0.         0.29626366]\n",
            "avg: 0.04868941174613105\n",
            "Comparing Result: [0.15692893 0.04660144 0.26298067]\n",
            "avg: 0.051834560102886625\n",
            "Comparing Result: [0.26632264 0.01581737 0.05047837]\n",
            "avg: 0.03695759839481778\n",
            "Comparing Result: [0.28388205 0.         0.        ]\n",
            "avg: 0.031542450189590454\n",
            "Comparing Result: [0.         0.14292741 0.22806387]\n",
            "avg: 0.04122125440173679\n",
            "Comparing Result: [0.09560282 0.37322545 0.05436122]\n",
            "avg: 0.058132165008121066\n",
            "Comparing Result: [0.         0.12377877 0.        ]\n",
            "avg: 0.013753196431530846\n",
            "Comparing Result: [0.04820496 0.3114357  0.1569949 ]\n",
            "avg: 0.05740394857194689\n",
            "Comparing Result: [0.06049038 0.07912537 0.12625737]\n",
            "avg: 0.02954145934846666\n",
            "Comparing Result: [0.26632264 0.0632695  0.02523918]\n",
            "avg: 0.039425704214307994\n",
            "Comparing Result: [0.28388205 0.         0.        ]\n",
            "avg: 0.031542450189590454\n",
            "Comparing Result: [0.32060122 0.14930966 0.03038312]\n",
            "avg: 0.055588225523630776\n",
            "Comparing Result: [0.14194103 0.         0.29626366]\n",
            "avg: 0.04868941174613105\n",
            "Comparing Result: [0.15692893 0.04660144 0.26298067]\n",
            "avg: 0.051834560102886625\n",
            "Comparing Result: [0.26632264 0.01581737 0.05047837]\n",
            "avg: 0.03695759839481778\n",
            "Comparing Result: [0.28388205 0.         0.        ]\n",
            "avg: 0.031542450189590454\n",
            "Comparing Result: [0.         0.14292741 0.22806387]\n",
            "avg: 0.04122125440173679\n",
            "Comparing Result: [0.09560282 0.37322545 0.05436122]\n",
            "avg: 0.058132165008121066\n",
            "Comparing Result: [0.         0.12377877 0.        ]\n",
            "avg: 0.013753196431530846\n",
            "Comparing Result: [0.04820496 0.3114357  0.1569949 ]\n",
            "avg: 0.05740394857194689\n",
            "Comparing Result: [0.06049038 0.07912537 0.12625737]\n",
            "avg: 0.02954145934846666\n",
            "Comparing Result: [0.26632264 0.0632695  0.02523918]\n",
            "avg: 0.039425704214307994\n",
            "Comparing Result: [0.28388205 0.         0.        ]\n",
            "avg: 0.031542450189590454\n",
            "Comparing Result: [0.32060122 0.14930966 0.03038312]\n",
            "avg: 0.055588225523630776\n",
            "Comparing Result: [0.14194103 0.         0.29626366]\n",
            "avg: 0.04868941174613105\n",
            "Comparing Result: [0.15692893 0.04660144 0.26298067]\n",
            "avg: 0.051834560102886625\n",
            "Comparing Result: [0.26632264 0.01581737 0.05047837]\n",
            "avg: 0.03695759839481778\n",
            "Comparing Result: [0.28388205 0.         0.        ]\n",
            "avg: 0.031542450189590454\n",
            "Comparing Result: [0.         0.14292741 0.22806387]\n",
            "avg: 0.04122125440173679\n",
            "Comparing Result: [0.09560282 0.37322545 0.05436122]\n",
            "avg: 0.058132165008121066\n",
            "Comparing Result: [0.         0.12377877 0.        ]\n",
            "avg: 0.013753196431530846\n",
            "Comparing Result: [0.04820496 0.3114357  0.1569949 ]\n",
            "avg: 0.05740394857194689\n",
            "Comparing Result: [0.06049038 0.07912537 0.12625737]\n",
            "avg: 0.02954145934846666\n",
            "Comparing Result: [0.26632264 0.0632695  0.02523918]\n",
            "avg: 0.039425704214307994\n",
            "Comparing Result: [0.28388205 0.         0.        ]\n",
            "avg: 0.031542450189590454\n",
            "Comparing Result: [0.32060122 0.14930966 0.03038312]\n",
            "avg: 0.055588225523630776\n",
            "Comparing Result: [0.14194103 0.         0.29626366]\n",
            "avg: 0.04868941174613105\n",
            "Comparing Result: [0.15692893 0.04660144 0.26298067]\n",
            "avg: 0.051834560102886625\n",
            "Comparing Result: [0.26632264 0.01581737 0.05047837]\n",
            "avg: 0.03695759839481778\n",
            "Comparing Result: [0.28388205 0.         0.        ]\n",
            "avg: 0.031542450189590454\n",
            "Comparing Result: [0.         0.14292741 0.22806387]\n",
            "avg: 0.04122125440173679\n",
            "Comparing Result: [0.09560282 0.37322545 0.05436122]\n",
            "avg: 0.058132165008121066\n",
            "Comparing Result: [0.         0.12377877 0.        ]\n",
            "avg: 0.013753196431530846\n",
            "Comparing Result: [0.04820496 0.3114357  0.1569949 ]\n",
            "avg: 0.05740394857194689\n",
            "Comparing Result: [0.06049038 0.07912537 0.12625737]\n",
            "avg: 0.02954145934846666\n",
            "Comparing Result: [0.26632264 0.0632695  0.02523918]\n",
            "avg: 0.039425704214307994\n",
            "Comparing Result: [0.28388205 0.         0.        ]\n",
            "avg: 0.031542450189590454\n",
            "Comparing Result: [0.32060122 0.14930966 0.03038312]\n",
            "avg: 0.055588225523630776\n",
            "Comparing Result: [0.14194103 0.         0.29626366]\n",
            "avg: 0.04868941174613105\n",
            "Comparing Result: [0.15692893 0.04660144 0.26298067]\n",
            "avg: 0.051834560102886625\n",
            "Comparing Result: [0.26632264 0.01581737 0.05047837]\n",
            "avg: 0.03695759839481778\n",
            "Comparing Result: [0.28388205 0.         0.        ]\n",
            "avg: 0.031542450189590454\n",
            "Comparing Result: [0.         0.14292741 0.22806387]\n",
            "avg: 0.04122125440173679\n",
            "Comparing Result: [0.09560282 0.37322545 0.05436122]\n",
            "avg: 0.058132165008121066\n",
            "Comparing Result: [0.         0.12377877 0.        ]\n",
            "avg: 0.013753196431530846\n",
            "Comparing Result: [0.04820496 0.3114357  0.1569949 ]\n",
            "avg: 0.05740394857194689\n",
            "Comparing Result: [0.06049038 0.07912537 0.12625737]\n",
            "avg: 0.02954145934846666\n",
            "Comparing Result: [0.26632264 0.0632695  0.02523918]\n",
            "avg: 0.039425704214307994\n",
            "Comparing Result: [0.28388205 0.         0.        ]\n",
            "avg: 0.031542450189590454\n",
            "Comparing Result: [0.32060122 0.14930966 0.03038312]\n",
            "avg: 0.055588225523630776\n",
            "Comparing Result: [0.14194103 0.         0.29626366]\n",
            "avg: 0.04868941174613105\n",
            "Comparing Result: [0.15692893 0.04660144 0.26298067]\n",
            "avg: 0.051834560102886625\n",
            "Comparing Result: [0.26632264 0.01581737 0.05047837]\n",
            "avg: 0.03695759839481778\n",
            "Comparing Result: [0.28388205 0.         0.        ]\n",
            "avg: 0.031542450189590454\n",
            "Comparing Result: [0.         0.14292741 0.22806387]\n",
            "avg: 0.04122125440173679\n",
            "Comparing Result: [0.09560282 0.37322545 0.05436122]\n",
            "avg: 0.058132165008121066\n",
            "Comparing Result: [0.         0.12377877 0.        ]\n",
            "avg: 0.013753196431530846\n",
            "Comparing Result: [0.04820496 0.3114357  0.1569949 ]\n",
            "avg: 0.05740394857194689\n",
            "Comparing Result: [0.06049038 0.07912537 0.12625737]\n",
            "avg: 0.02954145934846666\n",
            "Comparing Result: [0.26632264 0.0632695  0.02523918]\n",
            "avg: 0.039425704214307994\n",
            "Comparing Result: [0.28388205 0.         0.        ]\n",
            "avg: 0.031542450189590454\n",
            "Comparing Result: [0.32060122 0.14930966 0.03038312]\n",
            "avg: 0.055588225523630776\n",
            "Comparing Result: [0.14194103 0.         0.29626366]\n",
            "avg: 0.04868941174613105\n",
            "Comparing Result: [0.15692893 0.04660144 0.26298067]\n",
            "avg: 0.051834560102886625\n",
            "Comparing Result: [0.26632264 0.01581737 0.05047837]\n",
            "avg: 0.03695759839481778\n",
            "Comparing Result: [0.28388205 0.         0.        ]\n",
            "avg: 0.031542450189590454\n",
            "Comparing Result: [0.         0.14292741 0.22806387]\n",
            "avg: 0.04122125440173679\n",
            "Comparing Result: [0.09560282 0.37322545 0.05436122]\n",
            "avg: 0.058132165008121066\n",
            "Comparing Result: [0.         0.12377877 0.        ]\n",
            "avg: 0.013753196431530846\n",
            "0.040335284163021644\n",
            "===\n",
            "===\n",
            "===\n",
            "The perecntage of similarity is therefore in %: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLd4incIPxHS"
      },
      "source": [
        "#Since the average similarity index percentage of similarity when files \"2, 3, 4\" combined together and compared to 1.txt is 4%, we can conclude that Ms. Lakshmi Sankaran can be hired by Nat Geo's hiring committee "
      ]
    }
  ]
}